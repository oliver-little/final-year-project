\chapter{Design}

After conducting my review of previous work, I spent time producing a high-level design for my solution. In particular, I analysed what features it needed to have, what technologies I would use, and the overall architecture. The aim of this was to ensure that the limited development time I had was always spent producing the most useful features.

\section{MoSCoW Requirements}

Before considering specific technologies and frameworks, I produced a list of MoSCoW requirements. Each requirement in the table below has two extra columns. The first column represents whether the requirement is Functional (F) or Non-Functional (NF), and the second column represents requirements that Must (M), Should (S) or Could (C) be completed.

\begin{center}
	\begin{xltabular}{0.82\paperwidth}{ | p{1.5cm} | p{1.5cm} | X | } 
		\hline
		F / NF& Priority & Requirement Description \\ \hline
		
 		\multicolumn{3}{|c|}{Domain Specific Language - Expressions} \\ \hline
		F & M & The language must support 5 data types: integers, floats, booleans, strings and date-time objects. \\ \hline
		F & M & The language must allow users to reference a field in the current dataset. \\ \hline
		F & M & The language must allow users to reference a constant value, which can take one of the data types defined above. \\ \hline
		F & M & The language must support arithmetic operations like add, subtract, multiply, division and modulo. \\ \hline
		F & M & The language must support string slicing and concatenation. \\ \hline
		F & S & The language should utilise polymorphism in add operations to apply string concatenation, or arithmetic addition depending on the data types of the arguments. \\ \hline
		F & C & The language could be designed in such a way to allow the user to define their own functions. \\ \hline
		NF & S & The language should be intuitive to use, with SQL-like syntax. \\ \hline
		
		\multicolumn{3}{|c|}{Domain Specific Language - Comparisons} \\ \hline
		F & M & The user must be able to provide expressions as inputs to comparison operators. \\ \hline
		F & M & The language must support equals, and not equals comparisons \\ \hline
		F & M & The language must support inequalities, using numerical ordering for number types, and lexicographic ordering for strings. \\ \hline
		F & M & The language must support null and not null checks. \\ \hline
		F & S & The language should support string comparisons, including case sensitive and insensitive versions of contains, starts with, and ends with. \\ \hline
		F & S & The language should allow the user to combine multiple comparison criteria using $AND$ and $OR$ operators. \\ \hline
		
		\multicolumn{3}{|c|}{Data Processing} \\ \hline
		F & M & The system must allow the user to write queries in Python. \\ \hline
		F & M & The system must allow users to apply Select operations on datasets, applying custom expressions to the input data. \\ \hline
		F & M & The system must allow users to apply Filter operations on datasets, applying custom comparisons to the input data. \\ \hline
		F & M & The system must allow users to apply Group By operations on datasets, which take a number of expressions as unique keys, and a number of aggregate. \\ \hline
		F & M & The Group By operation must allow users to apply Minimum, Maximum, Sum and Count aggregate functions to Group By operations. \\ \hline
		F & C & The system could allow users to apply Distinct Count, String Aggregate, and Distinct String Aggregate aggregate functions to Group By operations. \\ \hline
		F & S & The system should allow users to join two datasets together according to custom criteria. \\ \hline
		NF & S & The complexities of the system should be hidden from the user; from their perspective the operation should be identical whether the user is running the code locally or over a cluster. \\ \hline
		
		\multicolumn{3}{|c|}{Cluster} \\ \hline
		F & M & The system must allow the user to upload source data to a permanent data store. \\ \hline
		F & M & The orchestrator node must split up the full query and delegate partial work to the worker nodes. \\ \hline
		F & M & The orchestrator node must collect partial results from the cluster nodes to produce the overall result for the user. \\ \hline
		F & M & The orchestrator node must handle worker node failures and other computation errors by reporting them to the user. \\ \hline
		F & S & The orchestrator should perform load balancing to ensure work is evenly distributed among all nodes. \\ \hline
		F & C & The orchestrator could handle worker node failures by redistributing work to active workers. \\ \hline
		F & M & The worker nodes must accept partial work, compute and return results to the orchestrator. \\ \hline
		F & M & The worker nodes must pull source data from the permanent data store. \\ \hline
		F & M & The worker nodes must report any computation errors to the orchestrator. \\ \hline
		F & S & The worker nodes should cache results for reuse in later queries. \\ \hline
		F & S & The worker nodes should spill data to disk storage when available memory is low. \\ \hline
	\end{xltabular}
\end{center}

\section{Design Choices}

From the MoSCoW requirements I had a number of decisions to make regarding what languages and technologies I would use to implement my solution.

\subsection{Language}

\paragraph{Frontend} 
Python was selected as the language of choice for the frontend. This is because the intended users of my solution are most experienced with Python and SQL, which should make it easier for them to adopt and use my solution.

\paragraph{Orchestrator and Worker Nodes}
Both the orchestrator and worker nodes would use the same language, which would reduce overhead as the same codebase could be used for both parts of the system. When selecting a language, I firstly had to decide whether I would use a language with automatic or manual garbage collection (GC). Choosing a manual GC language would theoretically allow for increased performance, but I also felt that it would slow my development down significantly, as I would have to spend time handling GC myself. Therefore, I ruled out these languages.

The remaining options were a range of object-oriented and functional languages. I knew that much of the implementation would require iterating over lists of items, and functional languages are strong at this because of their use of operations like $map$ and $reduce$. However, I did not have any experience using purely functional languages like Haskell for large software engineering projects, and so ruled these out. I also knew that the solution would need to perform large amounts of CPU intensive processing, so a language with strong support for parallelisation was preferable. This ruled out languages like Python and JavaScript which are largely single-threaded; both support some form of parallelisation, but much more manual intervention by the developer is required.

In the end, I chose Scala, which is built on top of Java. It features a mix of both object-oriented, and functional paradigms, with built-in support for parallelisation and threading. This would allow me to leverage my previous experience writing large solutions in object-oriented languages, while making use of the functional programming style when convenient. Furthermore, packages originally written for Java can run in Scala, and I felt this wider compatibility would be useful with my later design choices.
 
\subsection{Runtime}
\paragraph{Containerisation}
With the nature of my project being to produce a distributed system, the clear choice for how to run my code was within containers. Docker is by far the most popular option for creating images to run as containers, but is not suitable for running and managing large numbers of containers. For this, I would need a container orchestration tool, and there are two main options: Docker Swarm, and Kubernetes. Docker Swarm is more closely integrated within the Docker ecosystem, but Kubernetes is more widely used in industry. I felt that the experience I would gain learning to use Kubernetes would be more useful, and therefore chose it as my container orchestration tool.

\paragraph{Network Communication} 
I also had to select a method of communicating between the Python frontend, the orchestrator and the worker nodes. At first, I considered various REST API frameworks, but upon further research, found that a remote procedure call (RPC) framework would be more suited to my needs. This is because REST is resource-centric, providing a standard set of operations - create, read, update, delete (CRUD). My system is less resource-centric, and more focused on operations, so the CRUD model wouldn't fit the requirements correctly. In contrast, RPC frameworks are designed to allow function calls over a remote network, while hiding the complexity of the remote operation from the developer. My research into these frameworks found that they are usually not designed around a particular data model like REST, which would allow me to implement my own function calls. 

In the end, I chose gRPC, designed and maintained by Google, as the RPC framework for my solution. I did this for a number of reasons. Firstly, there are well-maintained implementations for both Python and Scala, which would make using it in all parts of my system straightforward. Secondly, it uses protocol buffers as its message system, which is a serialisation format also maintained by Google. Protocol buffers are designed to be extremely storage efficient, which would reduce network overhead compared to a solution that used something like XML or JSON. As protocol buffers are also a serialisation format, gRPC provides APIs to store messages on disk, which I felt might be a useful feature.


\subsection{Persistent Storage}
I had a wide range of options for implementing persistent data storage. The first key decision in this area was whether to design this myself, or use an existing storage solution. A custom solution would come with the benefit of being more closely integrated with the rest of the system, but at the cost of increased development time. I chose not to implement my own solution, as I felt that I was already limited on time, and designing a resilient persistent storage module is a large challenge in its own right.

There are a wide range of options that exist for persistent data storage. I considered a number of types of file systems and databases which I eventually elected not to use:
\begin{itemize}
	\item Single System SQL Databases \textit{(Microsoft SQL Server, PostgreSQL, MySQL)}: while this option would be fastest to start using due to my prior knowledge, and extensive support, the database would quickly become a bottleneck in my system, as the rate at which data can be read from the server would determine how quickly computations could be performed.
	\item Distributed File Systems \textit{(Hadoop Distributed File System)}: these provide a mechanism for storing files resiliently across a number of machines, which would reduce the bottleneck when reading data. However, they provide no straightforward way of querying the stored data, meaning I would have to implement this myself.
	\item Distributed NoSQL Databases \textit{(MongoDB, CouchDB)}: these are distributed, meaning the load of reading the data could be spread across a number of machines. However, the data I will be processing is strictly tabular in format, meaning I don't need the features of a NoSQL architecture (document formats), and it is likely to result in added complexity when retrieving data from the database.
\end{itemize}

\paragraph{Apache Cassandra} 
In the end, I selected Apache Cassandra %reference
as my persistent storage module. This has a number of benefits for the solution I am designing. Firstly, the data model is very similar to single-system SQL databases, and as such closely matches the data model I intend to use. Furthermore, it is a distributed database, so source data will be stored across a number of machines (nodes). This will allow me to spread the load when retrieving data from the database, reducing the impact of read speed. 

Aside from these benefits, the main reason I selected Cassandra as my persistent data store was because of how it handles partitioning data. Each node in the database is assigned a token range, which determines what records it holds. When data is inserted into the database, Cassandra hashes the primary key of each record, producing a 64-bit token that maps it to a node. A diagram showing this process is included below:

% diagram

Cassandra allows these tokens to be filtered in ranges as part of queries, so this will allow me to split up the data that each of the worker nodes pull in .

Cassandra can be run on Kubernetes using K8ssandra \cite{k8ssandra}. This is a tool which can be used to initialise, configure and manage Cassandra clusters. This is particularly useful because the Cassandra cluster can be configured to run on the same machines as the worker nodes. This enables the source data to be co-located with the workers that will actually perform the computation, reducing network latency when transferring the source data.

In terms of interfacing with the rest of the system, there are drivers for both Python %reference
and Java %reference.
which are maintained by one of the largest contributors to Apache Cassandra. The Java driver has a core module which provides basic functionality for making queries and receiving results from the database. There are submodules with more complex functionality including query builders, but I chose not to use these as I was unlikely to actually use the features, and did not want to handle the extra complexity these submodules would introduce.

There are also some Scala specific frameworks for executing Cassandra queries, including Phantom %reference
and Quill. %reference
Again, I chose not to use these as I felt the extra functionality was not required, and they may introduce extra complexity which could be difficult to debug.


\section{Architecture}

