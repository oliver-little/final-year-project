\chapter{Evaluation}\label{cha:evaluation}
This section will discuss a high level evaluation of the solution, and the project as a whole.

\section{Limitations}
The solution has limitations which could not be fixed due to time constraints, but further investigation into optimisations or alternative solutions may be able to improve or fix them. These are listed below.

As discussed in Chapter \ref{cha:testing}, the Group By operation, and the method of transferring data around the network could both be further optimised.

%Firstly, transferring data over the network is common in the distributed system model. In particular, this occurs when returning final result data to the orchestrator and frontend, as well as when cross-communicating between workers. However, the performance testing identified that this is one of the weakest areas of the implementation. Therefore, optimising this process would be a focus in future development.

%The current implementations of Group Bys are computationally correct, but not as efficient as they could be. Currently, when partial data for a partition is communicated between workers, all rows from the source data are sent over the network. As the data transfer solution also has relatively poor performance, sending unnecessary data between workers exacerbates the issue. It should be possible to partially compute each group by during the hash calculation phase, then only send the partially computed result, which is then compiled by the worker that is responsible for the final partition. 

Result data uses an extremely large amount of space when resident in memory. For example, a 100MB source data file can use up to 800MB of memory once stored. This is because of the container class described in Section \ref{sec:type-system}. However, this class is a core component of the DSL, as it ensures the type information is accessible at runtime.

The system's security is limited, which prevents its use in a production environment. The orchestrator has no authentication, and Cassandra only has basic username and password authentication.

Finally, as discovered in Chapter \ref{cha:testing}, the result collation algorithm cannot return large amounts of results, typically more than 1 million rows. All workers send results to the orchestrator, which forwards them to the frontend. However, as there are more workers than the single orchestrator, data enters the orchestrator faster than it leaves, meaning that with a large enough dataset, the orchestrator will run out of memory and crash.

\section{Further Work}
The nature of this project means that there is a large scope for future work and improvements. As discussed in Section \ref{sec:parallelisation-test}, different cluster layouts are more optimised for different kinds of queries. 
%Queries with less computation, or that run on smaller amounts of data are better applied to smaller clusters, while increasing the level of parallelisation is better when the query is larger or more complex.
A module that runs at the Kubernetes level, monitoring the utilisation of the cluster and the types of queries being executed may be able to improve computation times by adjusting the cluster layout.

The data store is currently used to assist computations by temporarily storing partial result data. However, the design would allow it to store results between queries, improving the computation time of repeated queries to the same dataset. Join operations are also not currently implemented, but would benefit from this improvement to the data store. 

The current error handling is designed to forward any errors to the frontend. In some situations, like if the Cassandra database is unresponsive, this is acceptable. For other errors, like if one worker is unresponsive, this can be handled by delegating the failed worker's partitions to others, without alerting the user at all.

Finally, Cassandra is currently only used for storage and partitioning. However, it is also a query engine, meaning some computations could be performed on Cassandra directly, improving query times by reducing the amount of data transferred. In particular, Filters on the source dataset, and Group Bys on the primary key are perfect candidates for this optimisation.

% Personal analysis?? not present in other projects
% - Software Engineering Processes - CI/CD
% - Architecture
% - Approach (MVP first)

\section{Conclusion}
The objective as stated in Chapter \ref{cha:intro} was to design a query processing engine for a distributed cluster of nodes. The types of queries possible in the system are numerous, and the data model allows easy implementation of new query types. While performance testing results showed that the solution requires further optimisation to truly compete with existing frameworks, they also showed that there is promise in the scalability of the solution. Furthermore, testing revealed interesting findings regarding the number of workers in the cluster, and raised the possibility of a stand-alone module for performing node management. 

A secondary goal was to design the frontend to be easy-to-use, with SQL-like syntax. The DSL meets this goal, and is one of the defining features of the tool, with \textit{FieldExpressions} and \textit{FieldComparisons} allowing complex data manipulations to be defined with relative ease. The implementation of Functions permits easy extensions within the type system's bounds. The frontend operates seamlessly for the user, hiding the background operation of the framework entirely. Furthermore, the pandas integration means that users can immediately start manipulating result data using tools already familiar to them.