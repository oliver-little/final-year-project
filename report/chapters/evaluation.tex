\chapter{Evaluation}\label{cha:evaluation}
Following the performance testing of the completed solution, this section will discuss a high level evaluation of the solution, and the project as a whole.

\section{Limitations}
The features of the solution have a number of limitations which could be improved upon in future. As discussed in Chapter \ref{cha:testing}, the implementation of the Group By operation, and the method of transferring data around the network could both be further optimised.

%Firstly, transferring data over the network is common in the distributed system model. In particular, this occurs when returning final result data to the orchestrator and frontend, as well as when cross-communicating between workers. However, the performance testing identified that this is one of the weakest areas of the implementation. Therefore, optimising this process would be a focus in future development.

%The current implementations of Group Bys are computationally correct, but not as efficient as they could be. Currently, when partial data for a partition is communicated between workers, all rows from the source data are sent over the network. As the data transfer solution also has relatively poor performance, sending unnecessary data between workers exacerbates the issue. It should be possible to partially compute each group by during the hash calculation phase, then only send the partially computed result, which is then compiled by the worker that is responsible for the final partition. 

Result data uses an extremely large amount of space when resident in memory. This is because each cell in a result dataset is wrapped in a class containing its type, as well as the value. This decision results in even small datasets taking up large amounts of memory. For example, a 100MB source data file can use up to 800MB of memory once loaded into the storage format. As discussed in the Implementation chapter, this approach is essential to the functionality of the DSL, as the type information is not accessible at runtime otherwise. Further investigation would be required to determine an improvement to this limitation.

Due to time constraints, the system's security is limited. There is no authentication to access the orchestrator, and Cassandra only has basic username and password requirements for data inserts. This is something that would need to be developed for the system to be integrated in a production environment.

Finally, as discovered in Chapter \ref{cha:testing}, the result collation algorithm cannot cope with returning extremely large amounts of results, typically more than 1 million rows. All workers send results to the orchestrator, which sends them to the frontend as fast as it can. However, as there are more workers than the single orchestrator, data arrives into the orchestrator faster than it leaves. This means that with a large enough dataset, the orchestrator will run out of memory and crash. Alternative solutions need to be considered to fix this, but due to time constraints this limitation was not able to be fixed.

\section{Further Work}
The nature of this project means that there is a large scope for future work and improvements. As discussed in the Testing chapter, different cluster layouts are more optimised for different kinds of queries. Queries with less computation, or that run on smaller amounts of data are better applied to smaller clusters, while increasing the level of parallelisation is better when the query is larger or more complex. Therefore, a module that runs at the Kubernetes level, monitoring the utilisation of the cluster and the types of queries being executed may be able to improve computation times by adjusting the cluster layout.

The data store is currently used to assist in the computation of queries by temporarily storing partial result data. However, its design means that it could also be used to store results between queries. This would improve the computation time of repeated queries to the same dataset, as the steps to reach the stored result would not have to be repeated each time. Join operations would also be able to use this improvement to the data store. They are currently not implemented in the solution, but are essential for many types of queries, particularly when relating two different datasets by a common key.

The error handling in the system is designed to send an error message to the Python frontend if anything goes wrong during computation. For some errors, like if the Cassandra database is unexpectedly not responsive, this is acceptable. For other errors, like if a worker node suddenly goes down, it should be possible to handle this error by delegating the failed worker's partition to others, without alerting the user of any failure in the first place.

Finally, the Cassandra database is currently only used as a permanent data store, and for splitting up the source data into partitions. However, it is a database in its own right, and some computations could be performed on Cassandra before sending any data at all, improving query times by reducing the amount of network transfer. In particular, any filter operation on the source dataset, and group by operations on the primary key are perfect candidates for this optimisation, as it could be implemented with minimal work.

% Personal analysis?? not present in other projects
% - Software Engineering Processes - CI/CD
% - Architecture
% - Approach (MVP first)

\section{Conclusion}
The objective as stated in Chapter \ref{cha:intro} was to design a query processing engine for a distributed cluster of nodes, to increase the speed of data processing for large datasets. The types of queries possible in the system are numerous, and \textit{DataSource} and \textit{Table} model allows easy implementation of new query types. While performance testing results showed that the solution requires further optimisation to truly compete with existing frameworks, they also showed that there is promise in the scalability of the solution should these optimisations be made. Furthermore, testing revealed interesting findings regarding the number of workers in the cluster, and raised the possibility of a stand-alone module for performing node management. 

A secondary goal was to design the frontend to be easy-to-use, with SQL-like syntax. The DSL meets this goal, and is one of the defining features of the tool, with the FieldExpression and FieldComparison system allowing complex data manipulations to be defined with relative ease. The implementation of the Function system permits new functions to be added easily within the bounds of the type system. The operation of the frontend is seamless to the user, as the background operation of the framework is entirely hidden on the other nodes. Furthermore, the pandas integration means that users can immediately start manipulating result data using tools already familiar to them.